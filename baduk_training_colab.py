# ë°”ë‘‘ AI í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Google Colab í™˜ê²½)
# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° â†’ SGF íŒŒì¼ ìˆ˜ì§‘ â†’ ë°°ì¹˜ í•™ìŠµ + ì²´í¬í¬ì¸íŠ¸ + 8ë°° ë°ì´í„°ì¦ê°•

import os
import sys
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import gzip
import shutil
from pathlib import Path
import json
from typing import List, Tuple
import pickle
from datetime import datetime

# ============================================================================
# 1. Google Colab í™˜ê²½ ì„¤ì •
# ============================================================================

def setup_google_drive():
    """Google Drive ì—°ê²°"""
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        drive_root = '/content/drive/MyDrive'
        print(f"âœ… Google Drive ì—°ê²° ì„±ê³µ: {drive_root}")
        return drive_root
    except ImportError:
        print("âš ï¸  Google Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©")
        return './data'

# ============================================================================
# 2. SGF íŒŒì¼ ìˆ˜ì§‘ ë° ë¶„ì„
# ============================================================================

def collect_sgf_files(drive_root: str) -> List[Path]:
    """
    baduk/Pro/1/, baduk/Pro/2/, ... ë“±ì—ì„œ ëª¨ë“  .sgf íŒŒì¼ ìˆ˜ì§‘
    """
    sgf_files = []
    base_path = Path(drive_root) / 'baduk' / 'Pro'
    
    if not base_path.exists():
        print(f"âš ï¸  ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {base_path}")
        return []
    
    # Nì€ ìì—°ìˆ˜ (1, 2, 3, ...)
    for n_folder in sorted(base_path.iterdir()):
        if n_folder.is_dir() and n_folder.name.isdigit():
            sgf_files.extend(n_folder.glob('*.sgf'))
    
    print(f"âœ… ë°œê²¬ëœ SGF íŒŒì¼: {len(sgf_files)}ê°œ")
    return sorted(sgf_files)

def ask_training_count(total_files: int) -> int:
    """í•™ìŠµí•  íŒŒì¼ ìˆ˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë´„"""
    print(f"\nì´ {total_files}ê°œì˜ SGF íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    print(f"í•™ìŠµì— ì‚¬ìš©í•  íŒŒì¼ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-{total_files}):")
    
    while True:
        try:
            count = int(input())
            if 1 <= count <= total_files:
                return count
            else:
                print(f"1-{total_files} ì‚¬ì´ì˜ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”.")
        except ValueError:
            print("ì •ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

# ============================================================================
# 3. SGF íŒŒì‹± í•¨ìˆ˜
# ============================================================================

def parse_sgf(sgf_path: Path) -> List[Tuple[np.ndarray, int]]:
    """
    SGF íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ (board_state, move) ìŒ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Returns:
        List[(board_state: (19, 19, 3), move: 0-360)]
    """
    try:
        with open(sgf_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # ê°„ë‹¨í•œ SGF íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•  ìˆ˜ ìˆìŒ)
        training_data = []
        
        # SGFì—ì„œ move sequence ì¶”ì¶œ
        move_list = extract_moves_from_sgf(content)
        
        if not move_list:
            return []
        
        # Board ì¬í˜„ ë° training data ìƒì„±
        board = np.zeros((19, 19, 3), dtype=np.float32)
        for move_idx, (row, col, color) in enumerate(move_list):
            if 0 <= row < 19 and 0 <= col < 19:
                # color: 0 = black, 1 = white
                board[row, col, color] = 1
                move_label = row * 19 + col
                training_data.append((board.copy(), move_label, color))
        
        return training_data
    
    except Exception as e:
        print(f"âŒ SGF íŒŒì‹± ì‹¤íŒ¨ ({sgf_path.name}): {e}")
        return []

def extract_moves_from_sgf(content: str) -> List[Tuple[int, int, int]]:
    """
    SGF ë‚´ìš©ì—ì„œ ì›€ì§ì„ ì¶”ì¶œ
    ë°˜í™˜: [(row, col, color), ...] where color is 0 (black) or 1 (white)
    """
    moves = []
    import re
    
    # Black moves: ;B[xx]
    black_pattern = r';B\[([a-s]{2})\]'
    white_pattern = r';W\[([a-s]{2})\]'
    
    # ìˆœì„œëŒ€ë¡œ ëª¨ë“  move ì°¾ê¸°
    game_pattern = r'\(.*?\)'
    games = re.findall(game_pattern, content, re.DOTALL)
    
    if not games:
        return []
    
    game = games[0]
    move_sequence = re.findall(r';([BW])\[([a-s]{2})\]', game)
    
    for color_char, coords in move_sequence:
        try:
            col = ord(coords[0]) - ord('a')
            row = ord(coords[1]) - ord('a')
            color = 0 if color_char == 'B' else 1
            moves.append((row, col, color))
        except:
            continue
    
    return moves

# ============================================================================
# 4. ë°ì´í„° ì¦ê°• (8ë°°)
# ============================================================================

def augment_data(board: np.ndarray, move: int) -> List[Tuple[np.ndarray, int]]:
    """
    ë°”ë‘‘íŒì˜ íšŒì „, ë°˜ì „ì„ ì´ìš©í•˜ì—¬ 8ë°° ë°ì´í„° ì¦ê°•
    
    Returns:
        8ê°œì˜ (augmented_board, augmented_move) ìŒ
    """
    augmented = []
    
    def transform_move(move, transform_type):
        """move ë²ˆí˜¸ë¥¼ ë³€í™˜"""
        row, col = move // 19, move % 19
        
        if transform_type == 0:  # original
            return move
        elif transform_type == 1:  # 90ë„ íšŒì „
            new_row, new_col = col, 18 - row
        elif transform_type == 2:  # 180ë„ íšŒì „
            new_row, new_col = 18 - row, 18 - col
        elif transform_type == 3:  # 270ë„ íšŒì „
            new_row, new_col = 18 - col, row
        elif transform_type == 4:  # ìˆ˜í‰ ë°˜ì „
            new_row, new_col = row, 18 - col
        elif transform_type == 5:  # ìˆ˜í‰ ë°˜ì „ + 90ë„
            new_row, new_col = 18 - col, 18 - row
        elif transform_type == 6:  # ìˆ˜í‰ ë°˜ì „ + 180ë„
            new_row, new_col = 18 - row, col
        elif transform_type == 7:  # ìˆ˜í‰ ë°˜ì „ + 270ë„
            new_row, new_col = col, row
        
        return new_row * 19 + new_col
    
    for i in range(8):
        if i == 0:  # original
            aug_board = board.copy()
        elif i == 1:  # 90ë„ íšŒì „
            aug_board = np.rot90(board, 1)
        elif i == 2:  # 180ë„ íšŒì „
            aug_board = np.rot90(board, 2)
        elif i == 3:  # 270ë„ íšŒì „
            aug_board = np.rot90(board, 3)
        elif i == 4:  # ìˆ˜í‰ ë°˜ì „
            aug_board = np.fliplr(board)
        elif i == 5:  # ìˆ˜í‰ ë°˜ì „ + 90ë„
            aug_board = np.rot90(np.fliplr(board), 1)
        elif i == 6:  # ìˆ˜í‰ ë°˜ì „ + 180ë„
            aug_board = np.rot90(np.fliplr(board), 2)
        elif i == 7:  # ìˆ˜í‰ ë°˜ì „ + 270ë„
            aug_board = np.rot90(np.fliplr(board), 3)
        
        aug_move = transform_move(move, i)
        augmented.append((aug_board, aug_move))
    
    return augmented

# ============================================================================
# 5. ëª¨ë¸ ì •ì˜
# ============================================================================

def create_policy_network(input_shape=(19, 19, 3)):
    """ë°”ë‘‘ ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
    model = keras.Sequential([
        layers.Conv2D(64, 3, padding='same', activation='relu', input_shape=input_shape),
        layers.Conv2D(64, 3, padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(128, 3, padding='same', activation='relu'),
        layers.Conv2D(128, 3, padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(256, 3, padding='same', activation='relu'),
        layers.Conv2D(256, 3, padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(361, activation='softmax')  # 19x19 = 361
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# ============================================================================
# 6. í•™ìŠµ ë°ì´í„° íŒŒì´í”„ë¼ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)
# ============================================================================

class StreamingTrainingPipeline:
    """íŒŒì‹± â†’ ì¦ê°• â†’ í•™ìŠµ â†’ ì‚­ì œë¥¼ ì—°ì†ì ìœ¼ë¡œ ì²˜ë¦¬"""
    
    def __init__(self, sgf_files: List[Path], batch_size: int = 32, augment_factor: int = 8):
        self.sgf_files = sgf_files
        self.batch_size = batch_size
        self.augment_factor = augment_factor
        self.data_buffer = []
        self.labels_buffer = []
    
    def process_batch(self, sgf_files_batch: List[Path]) -> Tuple[np.ndarray, np.ndarray]:
        """
        ë°°ì¹˜ì˜ SGF íŒŒì¼ë“¤ì„ íŒŒì‹± â†’ ì¦ê°•í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„±
        """
        self.data_buffer = []
        self.labels_buffer = []
        
        for sgf_file in sgf_files_batch:
            print(f"  íŒŒì‹± ì¤‘: {sgf_file.name}")
            training_data = parse_sgf(sgf_file)
            
            for board, move, color in training_data:
                # 8ë°° ì¦ê°•
                augmented_samples = augment_data(board, move)
                for aug_board, aug_move in augmented_samples:
                    self.data_buffer.append(aug_board)
                    self.labels_buffer.append(aug_move)
        
        # ë©”ëª¨ë¦¬ì— ë¡œë“œ
        X = np.array(self.data_buffer, dtype=np.float32)
        y = np.array(self.labels_buffer, dtype=np.int32)
        
        print(f"  ìƒì„±ëœ í•™ìŠµ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        self.data_buffer = []
        self.labels_buffer = []
        
        return X, y

# ============================================================================
# 7. ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê¹…
# ============================================================================

class TrainingLogger:
    """í•™ìŠµ ì§„í–‰ ìƒí™© ê¸°ë¡"""
    
    def __init__(self, log_dir: str = './training_logs'):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.log_file = self.log_dir / f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        self.logs = {
            'start_time': datetime.now().isoformat(),
            'batches': []
        }
    
    def log_batch(self, batch_num: int, loss: float, accuracy: float, samples: int):
        """ë°°ì¹˜ ê²°ê³¼ ê¸°ë¡"""
        self.logs['batches'].append({
            'batch': batch_num,
            'loss': float(loss),
            'accuracy': float(accuracy),
            'samples': samples,
            'timestamp': datetime.now().isoformat()
        })
        self._save()
    
    def _save(self):
        """ë¡œê·¸ ì €ì¥"""
        with open(self.log_file, 'w') as f:
            json.dump(self.logs, f, indent=2, ensure_ascii=False)

# ============================================================================
# 8. ë©”ì¸ í•™ìŠµ ë£¨í”„
# ============================================================================

def train_baduk_ai(drive_root: str, num_epochs: int = 3, batch_sgf_size: int = 300):
    """
    ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
    
    Args:
        drive_root: Google Drive ë£¨íŠ¸ ê²½ë¡œ
        num_epochs: ì—í¬í¬ ìˆ˜
        batch_sgf_size: í•œ ë²ˆì— í•™ìŠµí•  SGF íŒŒì¼ ìˆ˜
    """
    
    # 1. SGF íŒŒì¼ ìˆ˜ì§‘
    print("\n" + "="*60)
    print("ğŸ” ë‹¨ê³„ 1: SGF íŒŒì¼ ìˆ˜ì§‘")
    print("="*60)
    
    sgf_files = collect_sgf_files(drive_root)
    
    if not sgf_files:
        print("âŒ SGF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. í•™ìŠµí•  íŒŒì¼ ìˆ˜ ê²°ì •
    print("\n" + "="*60)
    print("â“ ë‹¨ê³„ 2: í•™ìŠµ ë°ì´í„° í¬ê¸° ê²°ì •")
    print("="*60)
    
    num_files = ask_training_count(len(sgf_files))
    sgf_files_to_train = sgf_files[:num_files]
    
    print(f"\nğŸ“Š í•™ìŠµ ê³„íš:")
    print(f"  - ì´ SGF íŒŒì¼: {num_files}ê°œ")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {batch_sgf_size}ê°œ íŒŒì¼")
    print(f"  - ë°°ì¹˜ ìˆ˜: {(num_files + batch_sgf_size - 1) // batch_sgf_size}ê°œ")
    print(f"  - ë°ì´í„° ì¦ê°•: 8ë°°")
    
    # 3. ëª¨ë¸ ìƒì„±
    print("\n" + "="*60)
    print("ğŸ¤– ë‹¨ê³„ 3: ëª¨ë¸ ìƒì„±")
    print("="*60)
    
    model_dir = Path(drive_root) / 'baduk_models'
    model_dir.mkdir(exist_ok=True)
    
    model = create_policy_network()
    print("âœ… ì •ì±… ë„¤íŠ¸ì›Œí¬ ìƒì„± ì™„ë£Œ")
    print(model.summary())
    
    # 4. íŒŒì´í”„ë¼ì¸ ë° ë¡œê±° ì´ˆê¸°í™”
    pipeline = StreamingTrainingPipeline(sgf_files_to_train, batch_size=32, augment_factor=8)
    logger = TrainingLogger(str(model_dir / 'logs'))
    
    # 5. ë°°ì¹˜ í•™ìŠµ
    print("\n" + "="*60)
    print("ğŸš€ ë‹¨ê³„ 4: ë°°ì¹˜ í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    total_batches = (num_files + batch_sgf_size - 1) // batch_sgf_size
    
    for batch_idx in range(total_batches):
        print(f"\nğŸ“š ë°°ì¹˜ {batch_idx + 1}/{total_batches}")
        print("-" * 60)
        
        # ë°°ì¹˜ ë²”ìœ„
        start_idx = batch_idx * batch_sgf_size
        end_idx = min((batch_idx + 1) * batch_sgf_size, num_files)
        batch_files = sgf_files_to_train[start_idx:end_idx]
        
        # ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        X_batch, y_batch = pipeline.process_batch(batch_files)
        
        if len(X_batch) == 0:
            print(f"âš ï¸  ë°°ì¹˜ {batch_idx + 1}ì—ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue
        
        # ëª¨ë¸ í•™ìŠµ
        print(f"\nğŸ”¥ í•™ìŠµ ì¤‘... ({len(X_batch)}ê°œ ìƒ˜í”Œ)")
        history = model.fit(
            X_batch, y_batch,
            epochs=num_epochs,
            batch_size=32,
            validation_split=0.1,
            verbose=1
        )
        
        # ë¡œê·¸ ì €ì¥
        final_loss = history.history['loss'][-1]
        final_acc = history.history['accuracy'][-1]
        logger.log_batch(batch_idx + 1, final_loss, final_acc, len(X_batch))
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = model_dir / f'checkpoint_batch_{batch_idx + 1:03d}.h5'
        model.save(str(checkpoint_path))
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path.name}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del X_batch, y_batch
        import gc
        gc.collect()
        
        print(f"âœ… ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ (Loss: {final_loss:.4f}, Acc: {final_acc:.4f})")
    
    # 6. ìµœì¢… ëª¨ë¸ ì €ì¥
    print("\n" + "="*60)
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print("="*60)
    
    final_model_path = model_dir / 'baduk_ai_final.h5'
    model.save(str(final_model_path))
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # í•™ìŠµ ìš”ì•½ ì¶œë ¥
    print(f"\nğŸ“Š í•™ìŠµ ìš”ì•½:")
    print(f"  - ì´ SGF íŒŒì¼ ì²˜ë¦¬: {num_files}ê°œ")
    print(f"  - ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
    print(f"  - ìµœì¢… ëª¨ë¸ ê²½ë¡œ: {final_model_path}")
    print(f"  - ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {model_dir}")
    print(f"  - ë¡œê·¸ ê²½ë¡œ: {logger.log_file}")

# ============================================================================
# 9. ì‹¤í–‰
# ============================================================================

print("\nğŸ´ ë°”ë‘‘ AI í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
print("=" * 60)

# Google Drive ì—°ê²°
drive_root = setup_google_drive()

# í•™ìŠµ ì‹¤í–‰
train_baduk_ai(
    drive_root=drive_root,
    num_epochs=3,           # ì—í¬í¬
    batch_sgf_size=300      # ë°°ì¹˜ë‹¹ SGF íŒŒì¼ ìˆ˜
)

print("\n" + "="*60)
print("âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
print("="*60)
